{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOJqiEnxQVAn"
   },
   "source": [
    "# Assignment 1\n",
    "    Created by: Group F - Gaurav, Xiaowen Sun, Jheel Harnish Kamdar, Ruijia Xiong\n",
    "    \n",
    "    Created at: 01/23/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itdFNZSwQVAq"
   },
   "source": [
    "### ** Highlights**\n",
    "\n",
    "1. Through our observation, AGE and SEX plays an extremely important role for clustering.\n",
    "    - Patient 0 and 6 have exactly the same AGE and SEX: 17 year old male\n",
    "    - Patient 1 and 9 have exactly the same AGE and SEX: 20 year old female\n",
    "    - Patient 3 and 7 have the same AGE: 35 year old\n",
    "    \n",
    "    In our previous code running, we notice that these labelled groups are easily clustered with each other, so we think AGE and SEX play very important role for clustering. That makes us formatter AGE and SEX specifically.\n",
    "\n",
    "    We wrote a dictionary for each patient. However, it's worth noticing this method is not replicable once the number of patients increase.\n",
    "2. Although there are 40,000 data points in the dataset, we should understand that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y24qMjGQQVAr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import symspellpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yT6zlLcLbQhx",
    "outputId": "0c091c5d-5f40-49c4-ace0-99fea0e65840"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xiong.ru/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/xiong.ru/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sClW2Rh8QVAt"
   },
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "fj4t3DtNQVAt",
    "outputId": "d045d8d4-703c-4ee9-9374-dbe88774977f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42146, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17 yo male with recurrent palpitations for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17yo male with no pmh here for evaluation of p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pn_num  case_num                                         pn_history\n",
       "0       0         0  17-year-old male, has come to the student heal...\n",
       "1       1         0  17 yo male with recurrent palpitations for the...\n",
       "2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n",
       "3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
       "4       4         0  17yo male with no pmh here for evaluation of p..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('patient_notes.csv')\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOLGJrJMQVAu"
   },
   "source": [
    "### 1.1 Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D6ix3wERQVAu"
   },
   "outputs": [],
   "source": [
    "df['pn_history'] = df['pn_history'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUVthi3FQVAv"
   },
   "source": [
    "### 1.2 Handling Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LuL1uZ3XQVAv"
   },
   "outputs": [],
   "source": [
    "# Use Regex to handle contractions\n",
    "def decontracted(text):\n",
    "    # specific\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"yo\", \" year old \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['pn_history'] = df['pn_history'].apply(decontracted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLMGpaAVQVAw"
   },
   "source": [
    "### 1.3 Removing Punctuation and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F6QZT8W0QVAw"
   },
   "outputs": [],
   "source": [
    "df['pn_history'] = df['pn_history'].str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5xEdbu_QVAw"
   },
   "source": [
    "### 1.4 Standardizing Formats\n",
    "Standardizing formats for age, sex, dates, numbers, and currencies\n",
    "\n",
    "    • After our observation, almost each note recorded patient's age and sex, so we're going to format/standardize age and sex\n",
    "    • Since we only have ten patients in total, so we only have 10 different age+sex combination:\n",
    "        0. 17 year old male\n",
    "        1. 20 year old female\n",
    "        2. 44 year old female\n",
    "        3. 35 year old male\n",
    "        4. 54 year old female\n",
    "        5. 26 year old female\n",
    "        **6. 17 year old male\n",
    "        7. 35 year old female\n",
    "        8. 67 year old female\n",
    "        **9. 20 year old female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3PnNW7JQVAx"
   },
   "source": [
    "#### 1.4.1 Standardize AGE and SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VjHvZfY8QVAx"
   },
   "outputs": [],
   "source": [
    "# Function to standardize age and sex formats\n",
    "def standardize_age(df):\n",
    "    age_and_sex = {\n",
    "    \"variations_0\" : [\"17yearold male\", \"17yearold m\", \"17yearold boy\", \"17yearold b\", \"17yearold man\", \"17 year old male\", \"17 year old m\", \"17 year old boy\", \"17 year old b\", \"17 year old man\", \"17 yo male\", \"17 yo m\", \"17 yo boy\", \"17 yo b\", \"17 yo man\", \"17yo male\", \"17yo m\", \"17yo boy\", \"17yo b\", \"17yo man\", \"17 yr male\", \"17 yr m\", \"17 yr boy\", \"17 yr b\", \"17 yr man\", \"17 y o male\", \"17 y o m\", \"17 y o boy\", \"17 y o b\", \"17 y o man\", \"17yomale\", \"17yom\", \"17yoboy\", \"17yob\", \"17yoman\"],\n",
    "    \"variations_1\" : [\"20yearold female\", \"20yearold f\", \"20yearold girl\", \"20yearold g\", \"20yearold woman\", \"20 year old female\", \"20 year old f\", \"20 year old girl\", \"20 year old g\", \"20 year old woman\", \"20 yo female\", \"20 yo f\", \"20 yo girl\", \"20 yo g\", \"20 yo woman\", \"20yo female\", \"20yo f\", \"20yo girl\", \"20yo g\", \"20yo woman\", \"20 yr female\", \"20 yr f\", \"20 yr girl\", \"20 yr g\", \"20 yr woman\", \"20 y o female\", \"20 y o f\", \"20 y o girl\", \"20 y o g\", \"20 y o woman\", \"20yofemale\", \"20yof\", \"20yogirl\", \"20yog\", \"20yowoman\"],\n",
    "    \"variations_2\" : [\"44yearold female\", \"44yearold f\", \"44yearold girl\", \"44yearold g\", \"44yearold woman\", \"44 year old female\", \"44 year old f\", \"44 year old girl\", \"44 year old g\", \"44 year old woman\", \"44 yo female\", \"44 yo f\", \"44 yo girl\", \"44 yo g\", \"44 yo woman\", \"44yo female\", \"44yo f\", \"44yo girl\", \"44yo g\", \"44yo woman\", \"44 yr female\", \"44 yr f\", \"44 yr girl\", \"44 yr g\", \"44 yr woman\", \"44 y o female\", \"44 y o f\", \"44 y o girl\", \"44 y o g\", \"44 y o woman\", \"44yofemale\", \"44yof\", \"44yogirl\", \"44yog\", \"44yowoman\"],\n",
    "    \"variations_3\" : [\"35yearold male\", \"35yearold m\", \"35yearold boy\", \"35yearold b\", \"35yearold man\", \"35 year old male\", \"35 year old m\", \"35 year old boy\", \"35 year old b\", \"35 year old man\", \"35 yo male\", \"35 yo m\", \"35 yo boy\", \"35 yo b\", \"35 yo man\", \"35yo male\", \"35yo m\", \"35yo boy\", \"35yo b\", \"35yo man\", \"35 yr male\", \"35 yr m\", \"35 yr boy\", \"35 yr b\", \"35 yr man\", \"35 y o male\", \"35 y o m\", \"35 y o boy\", \"35 y o b\", \"35 y o man\", \"35yomale\", \"35yom\", \"35yoboy\", \"35yob\", \"35yoman\"],\n",
    "    \"variations_4\" : [\"54yearold female\", \"54yearold f\", \"54yearold girl\", \"54yearold g\", \"54yearold woman\", \"54 year old female\", \"54 year old f\", \"54 year old girl\", \"54 year old g\", \"54 year old woman\", \"54 yo female\", \"54 yo f\", \"54 yo girl\", \"54 yo g\", \"54 yo woman\", \"54yo female\", \"54yo f\", \"54yo girl\", \"54yo g\", \"54yo woman\", \"54 yr female\", \"54 yr f\", \"54 yr girl\", \"54 yr g\", \"54 yr woman\", \"54 y o female\", \"54 y o f\", \"54 y o girl\", \"54 y o g\", \"54 y o woman\", \"54yofemale\", \"54yof\", \"54yogirl\", \"54yog\", \"54yowoman\"],\n",
    "    \"variations_5\" : [\"26yearold female\", \"26yearold f\", \"26yearold girl\", \"26yearold g\", \"26yearold woman\", \"26 year old female\", \"26 year old f\", \"26 year old girl\", \"26 year old g\", \"26 year old woman\", \"26 yo female\", \"26 yo f\", \"26 yo girl\", \"26 yo g\", \"26 yo woman\", \"26yo female\", \"26yo f\", \"26yo girl\", \"26yo g\", \"26yo woman\", \"26 yr female\", \"26 yr f\", \"26 yr girl\", \"26 yr g\", \"26 yr woman\", \"26 y o female\", \"26 y o f\", \"26 y o girl\", \"26 y o g\", \"26 y o woman\", \"26yofemale\", \"26yof\", \"26yogirl\", \"26yog\", \"26yowoman\"],\n",
    "    \"variations_7\" : [\"35yearold female\", \"35yearold f\", \"35yearold girl\", \"35yearold g\", \"35yearold woman\", \"35 year old female\", \"35 year old f\", \"35 year old girl\", \"35 year old g\", \"35 year old woman\", \"35 yo female\", \"35 yo f\", \"35 yo girl\", \"35 yo g\", \"35 yo woman\", \"35yo female\", \"35yo f\", \"35yo girl\", \"35yo g\", \"35yo woman\", \"35 yr female\", \"35 yr f\", \"35 yr girl\", \"35 yr g\", \"35 yr woman\", \"35 y o female\", \"35 y o f\", \"35 y o girl\", \"35 y o g\", \"35 y o woman\", \"35yofemale\", \"35yof\", \"35yogirl\", \"35yog\", \"35yowoman\"],\n",
    "    \"variations_8\" : [\"67yearold female\", \"67yearold f\", \"67yearold girl\", \"67yearold g\", \"67yearold woman\", \"67 year old female\", \"67 year old f\", \"67 year old girl\", \"67 year old g\", \"67 year old woman\", \"67 yo female\", \"67 yo f\", \"67 yo girl\", \"67 yo g\", \"67 yo woman\", \"67yo female\", \"67yo f\", \"67yo girl\", \"67yo g\", \"67yo woman\", \"67 yr female\", \"67 yr f\", \"67 yr girl\", \"67 yr g\", \"67 yr woman\", \"67 y o female\", \"67 y o f\", \"67 y o girl\", \"67 y o g\", \"67 y o woman\", \"67yofemale\", \"67yof\", \"67yogirl\", \"67yog\", \"67yowoman\"]\n",
    "    }\n",
    "    age_and_sex_conversion = {\"variations_0\": \"17 year old male\", \"variations_1\": \"20 year old female\", \"variations_2\": \"44 year old female\", \"variations_3\": \"35 year old male\", \"variations_4\": \"54 year old female\", \"variations_5\": \"26 year old female\", \"variations_7\": \"35 year old female\", \"variations_8\": \"67 year old female\"}\n",
    "\n",
    "    age = {\n",
    "    \"variations_age_0\" : [\"17yearold\", \"17 yo\", \"17yo\", \"17yr\", \"17 yr\", \"17 y o\", '17 y r'],\n",
    "    \"variations_age_1\" : [\"20yearold\", \"20 yo\", \"20yo\", \"20yr\", \"20 yr\", \"20 y o\", '20 y r'],\n",
    "    \"variations_age_2\" : [\"44yearold\", \"44 yo\", \"44yo\", \"44yr\", \"44 yr\", \"44 y o\", '44 y r'],\n",
    "    \"variations_age_3\" : [\"35yearold\", \"35 yo\", \"35yo\", \"35yr\", \"35 yr\", \"35 y o\", '35 y r'],\n",
    "    \"variations_age_4\" : [\"54yearold\", \"54 yo\", \"54yo\", \"54yr\", \"54 yr\", \"54 y o\", '54 y r'],\n",
    "    \"variations_age_5\" : [\"26yearold\", \"26 yo\", \"26yo\", \"26yr\", \"26 yr\", \"26 y o\", '26 y r'],\n",
    "    \"variations_age_8\" : [\"67yearold\", \"67 yo\", \"67yo\", \"67yr\", \"67 yr\", \"67 y o\", '67 y r']\n",
    "    }\n",
    "    age_conversion = {\"variations_age_0\" : '17 year old', \"variations_age_1\" : '20 year old', \"variations_age_2\" : '44 year old', \"variations_age_3\" : '35 year old', \"variations_age_4\" : '54 year old', \"variations_age_5\" : '26 year old', \"variations_age_8\" : '67 year old'}\n",
    "\n",
    "    for i in [0,1,2,3,4,5,7,8]:\n",
    "        pattern = '|'.join(map(re.escape, age_and_sex[f'variations_{i}']))\n",
    "        pattern = re.compile(pattern)\n",
    "        df['pn_history'] = df['pn_history'].apply(lambda x: pattern.sub(age_and_sex_conversion[f'variations_{i}'], x))\n",
    "\n",
    "\n",
    "    for i in [0,1,2,3,4,5,8]:\n",
    "        pattern = '|'.join(map(re.escape, age[f'variations_age_{i}']))\n",
    "        pattern = re.compile(pattern)\n",
    "        df['pn_history'] = df['pn_history'].apply(lambda x: pattern.sub(age_conversion[f'variations_age_{i}'], x))\n",
    "\n",
    "standardize_age(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npi1UK1sQVAy"
   },
   "source": [
    "#### 1.4.2 Standardize DATE, NUMBER, AND CURRENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-lvY5Pl5QVAz"
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "\n",
    "# Define a function to standardize date formats\n",
    "def standardize_dates(text):\n",
    "    try:\n",
    "        # Try to parse and reformat dates found in the text\n",
    "        return parse(text, fuzzy=True).strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        # If parsing fails, return the original text\n",
    "        return text\n",
    "\n",
    "# Define a function to standardize numbers (remove commas from large numbers)\n",
    "def standardize_numbers(text):\n",
    "    return re.sub(r'(\\d{1,3}),(\\d{3}\\b)', r'\\1\\2', text)\n",
    "\n",
    "# Define a function to standardize currency symbols to their text equivalents\n",
    "def standardize_currencies(text):\n",
    "    # Replace the US Dollar symbol with 'USD'\n",
    "    text = re.sub(r'\\$', 'USD ', text)\n",
    "    # Replace the British Pound symbol with 'GBP'\n",
    "    text = re.sub(r'£', 'GBP ', text)\n",
    "    # Replace the Euro symbol with 'EUR'\n",
    "    text = re.sub(r'€', 'EUR ', text)\n",
    "    return text\n",
    "\n",
    "# Apply the function to standardize dates in each note\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', lambda match: standardize_dates(match.group()), x))\n",
    "# Apply the function to standardize numbers in each note\n",
    "df['pn_history'] = df['pn_history'].apply(standardize_numbers)\n",
    "# Apply the function to standardize currencies in each note\n",
    "df['pn_history'] = df['pn_history'].apply(standardize_currencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7WWjAuKQVAz"
   },
   "source": [
    "### 1.5 Correcting Typos and Spelling\n",
    "1. Spell Correction:\n",
    "    - We use SymSpell library to find the correct spelling by considering various possibilities of the possible typos of a word/gram.\n",
    "\n",
    "    - SymSpell calculates the edit distance (the number of insertions, deletions, substitutions, or transpositions needed to transform one word into another) between the input word and all words in the dictionary.\n",
    "\n",
    "    - Words in the dictionary that are within a certain edit distance threshold are considered potential corrections.\n",
    "2. Candidate Generation:\n",
    "    - SymSpell generates a list of candidate corrections based on the edit distance calculations. These candidates are potential correct spellings for the input word.\n",
    "    \n",
    "    - The candidates are sorted by their frequency in the dictionary, with more frequent words given higher priority.\n",
    "\n",
    "    - Then we use the priority gram/words to substitute the possible typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq_WzQ4HQVAz"
   },
   "source": [
    "#### 1.5.1 Create typo frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gSbJHJTrQVAz"
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # # Remove single-character tokens\n",
    "    # tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Create a frequency dictionary\n",
    "def create_frequency_dictionary(tokens):\n",
    "    frequency_dict = collections.Counter(tokens)\n",
    "    return frequency_dict\n",
    "\n",
    "# Create a bigram dictionary\n",
    "def create_bigram_dictionary(tokens):\n",
    "    bigram_dict = collections.Counter(zip(tokens, tokens[1:]))\n",
    "    return bigram_dict\n",
    "\n",
    "# Tokenize and preprocess the 'pn_history' column\n",
    "tokenized_pn_history = df['pn_history'].apply(custom_tokenizer)\n",
    "\n",
    "# Flatten the list of tokenized notes into a single list of tokens\n",
    "all_tokens = [token for sublist in tokenized_pn_history for token in sublist]\n",
    "\n",
    "# Create frequency dictionary\n",
    "frequency_dictionary = create_frequency_dictionary(all_tokens)\n",
    "\n",
    "# Create bigram dictionary\n",
    "bigram_dictionary = create_bigram_dictionary(all_tokens)\n",
    "\n",
    "# Save frequency dictionary to a text file\n",
    "with open(\"frequency_dictionary.txt\", \"w\") as f:\n",
    "    for word, count in frequency_dictionary.items():\n",
    "        f.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "# Save bigram dictionary to a text file\n",
    "with open(\"bigram_dictionary.txt\", \"w\") as f:\n",
    "    for bigram, count in bigram_dictionary.items():\n",
    "        f.write(f\"{bigram[0]} {bigram[1]}: {count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEMMGoPJQVA0"
   },
   "source": [
    "#### 1.5.2 Substitute the typos with the priority words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RU9LLNraQVA0",
    "outputId": "4a94cec7-330d-4f1c-8048-f223fa8f6fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        17 year old male has come to the student healt...\n",
      "1        17 year old male with recurrent palpitations f...\n",
      "2        dillon cleveland is a 17 year old male patient...\n",
      "3        a 17 year old m co palpitation started 3 mo ag...\n",
      "4        17 year old male with no pmh her for evaluatio...\n",
      "                               ...                        \n",
      "42141    m madden is a 20 year old female presenting w ...\n",
      "42142    a 20 year old f came complain a dull 810 heada...\n",
      "42143    m madden is a 20 year old female who presents ...\n",
      "42144    stephanie madden is a 20 year old female compl...\n",
      "42145    patient is a 20 year old f who presents with a...\n",
      "Name: pn_history, Length: 42146, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create a SymSpell object\n",
    "symspell = symspellpy.SymSpell()\n",
    "\n",
    "# Load a frequency dictionary\n",
    "dictionary_path = \"frequency_dictionary.txt\"\n",
    "bigram_path = \"bigram_dictionary.txt\"\n",
    "\n",
    "# Load the dictionary and bigram data\n",
    "symspell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "symspell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# Function for spelling correction using SymSpell\n",
    "def correct_spelling_symspell(text):\n",
    "    # Tokenize the input text using regular expressions (split on non-word characters)\n",
    "    words = re.findall(r'\\w+', text)\n",
    "\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        # Check if the word is entirely composed of digits\n",
    "        if word.isdigit():\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            # Correct the spelling of the word using SymSpell\n",
    "            corrected_word = symspell.lookup(word, symspellpy.Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)[0].term\n",
    "            corrected_word = corrected_word.replace(':', '')  # Remove colons if added during correction\n",
    "            corrected_words.append(corrected_word)\n",
    "\n",
    "    # Join the corrected words back into a text with spaces\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "# Apply the spelling correction function to the 'pn_history' column\n",
    "df['pn_history'] = df['pn_history'].apply(correct_spelling_symspell)\n",
    "print(df['pn_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J41IsJkQVA0"
   },
   "source": [
    "### 1.6 Stemming and Lemmatization\n",
    "\n",
    "1. We tried to use both stemming and lemmatization, however, it doesn't seem to do much work;\n",
    "2. So we decide to comment out the code and not use stemming and lemmatization in our final code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cuStfdcRQVA0"
   },
   "outputs": [],
   "source": [
    "# # Initialize stemmer and lemmatizer\n",
    "# stemmer = PorterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# def stem(text):\n",
    "#     return ' '.join([stemmer.stem(word) for word in re.findall(r'\\w+', text)])\n",
    "\n",
    "# def lemma(text):\n",
    "#     return ' '.join([lemmatizer.lemmatize(word) for word in re.findall(r'\\w+', text)])\n",
    "\n",
    "\n",
    "# df['pn_history'] = df['pn_history'].apply(stem)\n",
    "# df['pn_history'] = df['pn_history'].apply(lemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V9pOaEnQVA0"
   },
   "source": [
    "### 1.7 Apply a stop word list to filter out unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CeymqTknQVA0"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KeKRQKiQVA1"
   },
   "source": [
    "## 2. Document-Term Matrix (DTM) Creation\n",
    "1. We're using TF-IDF vectorizer to create the DTM.\n",
    "2. We experiment with two versions of TF-IDF vectorizer:\n",
    "  - Without max_df and min_df\n",
    "  - With max_df and min_df limitations\n",
    "3. From the below code, it's obvious we should set up the min_df and max_df parameters in the TF-IDF vectorizer. Here are the reasons:\n",
    "  - For a data frame of 40,000 rows, it doesn't make sense to have 50,000 to 60,000 features;\n",
    "  - It makes much sense to keep the second vectorizer where there are only 9,700 feature columns;\n",
    "  - Although for vectorizer_1, we can claim that meaningless features can be dropped after using dimensionality reduction analysis. However, dimensionality algorithms are computationally much more expensive than TR-IDF vectorizer.\n",
    "  - In our vectorizer_2, we set the min_df to 5 in 40,000 rows, the parameter is very meaningful. If a certain word showed up 5 times in total in a 40,000 dataset, we can see their existence as accident, which won't do much help for our feature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "akW3hUmlQVA1"
   },
   "outputs": [],
   "source": [
    "# Initialize vecotrizer\n",
    "vectorizer_1 = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "\n",
    "# Initialize vecotrizer\n",
    "vectorizer_2 = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    max_df=0.85,         # Ignore terms that have a document frequency higher than 85%\n",
    "    min_df=5           # Ignore terms that have a document frequency lower than 5 documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KmxZ1PTQVA1"
   },
   "source": [
    "### 2.1 Create the DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzqDGsRzQVA1",
    "outputId": "4588b9a8-ee7c-42a8-ba5a-9122204a7ba9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiong.ru/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vectorizer_1, the dtm data frame shape: (42146, 57078)\n",
      "Using vectorizer_2, the dtm data frame shape: (42146, 9633)\n"
     ]
    }
   ],
   "source": [
    "pn_history_vector_1 = vectorizer_1.fit_transform(df['pn_history'])\n",
    "pn_history_vector_2 = vectorizer_2.fit_transform(df['pn_history'])\n",
    "print(f'Using vectorizer_1, the dtm data frame shape: {pn_history_vector_1.shape}')\n",
    "print(f'Using vectorizer_2, the dtm data frame shape: {pn_history_vector_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNRxAYLsQVA1"
   },
   "source": [
    "### 2.2 Describe the DTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_aBv8ekQVA1"
   },
   "source": [
    "1. As we can tell from the above shapes of the dtm:\n",
    "    - There's a huge difference between using and not using stop_words(max_df, min_df, max_features);\n",
    "    - Not using stop_words(feature selection), we have 57789 features;\n",
    "    - Using stop words leave us with only 9784 features\n",
    "2. For a dataset that only has 40,000 rows, use 57,789 features doesn't make sense at all, so we're going to stick with the second solution to remove the stop words(the most frequent ones and the least frequent ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tn7WlwaQVA1",
    "outputId": "c6f96642-a7c6-438a-9943-70ac5d2bb596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 9633\n",
      "First 100 features: \n",
      "['0' '00' '010' '02' '04' '05' '051' '0510' '051pack' '051packday'\n",
      " '051pkday' '051ppd' '051ppd20' '052' '05ppd' '1' '10' '100' '1000' '1008'\n",
      " '1010' '1012' '1014' '1015' '1015lb' '1015lbs' '1015pounds' '102' '1020'\n",
      " '1020pack' '1030' '1030pm' '1045' '105' '10lb' '10min' '10mo' '10pm'\n",
      " '10pound' '10py' '10th' '10y' '10years' '10yrs' '11' '110' '1100' '1112'\n",
      " '112' '115' '115ppd' '11am' '11pm' '11th' '11yrs' '12' '121' '1213'\n",
      " '121pack' '121packday' '121pk' '121pkday' '121ppd' '121ppd20' '122'\n",
      " '1224' '125' '125mg' '125mgday' '12am' '12beers' '12beersweek'\n",
      " '12beerswk' '12day' '12drinks' '12glasses' '12h' '12hours' '12hrs' '12mo'\n",
      " '12month' '12pack' '12pm' '12ppd' '12ppd1ppd' '12th' '12times'\n",
      " '12timesday' '12week' '12wk' '12x' '12xday' '12xmo' '12xmonth' '12xweek'\n",
      " '12xwk' '12xyear' '12y' '12year' '12years']\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer_2.get_feature_names_out()\n",
    "\n",
    "# Print the number of features (tokens)\n",
    "print(\"Number of features:\", len(feature_names))\n",
    "\n",
    "# Print the first 100 features\n",
    "print(f\"First 100 features: \\n{feature_names[0:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MW2qIXiqQVA1",
    "outputId": "fa74e3e5-1b1a-4967-beef-b3c564685676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total element numbers:  405,992,418\n",
      "Number of non-zero elements:  3,179,315; \n",
      "Proportion of non-zero elements:  0.00783\n",
      "Size in bytes:  3097.48 MB\n",
      "Sparsity of DTM:  0.99\n"
     ]
    }
   ],
   "source": [
    "# Total elements in the dtm\n",
    "total_elements = pn_history_vector_2.shape[0] * pn_history_vector_2.shape[1]\n",
    "print(f\"Total element numbers: {total_elements: ,}\")\n",
    "\n",
    "# Number of non-zero elements in the DTM\n",
    "non_zero_elements = pn_history_vector_2.getnnz()\n",
    "print(f\"Number of non-zero elements: {non_zero_elements: ,}; \\nProportion of non-zero elements: {non_zero_elements/total_elements : .5f}\")\n",
    "\n",
    "\n",
    "size_in_bytes = pn_history_vector_2.toarray().nbytes\n",
    "print(f\"Size in bytes: {(size_in_bytes/(1024*1024)): .2f} MB\")\n",
    "\n",
    "# Sparsity of the DTM\n",
    "sparsity = 1.0 - (non_zero_elements / (pn_history_vector_2.shape[0] * pn_history_vector_2.shape[1]))\n",
    "print(f\"Sparsity of DTM: {sparsity: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "6dy1CMJzQVA2",
    "outputId": "ce948008-dbf2-4ea1-d10d-7656ef66d1a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>010</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>051</th>\n",
       "      <th>0510</th>\n",
       "      <th>051pack</th>\n",
       "      <th>051packday</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>yeterday</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yo</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yrsno</th>\n",
       "      <th>ysterday</th>\n",
       "      <th>zero</th>\n",
       "      <th>zolpidem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42141</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42142</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42143</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42146 rows × 9633 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   00  010   02        04   05  051  0510  051pack  051packday  ...  \\\n",
       "0      0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "1      0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "2      0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "3      0.0  0.0  0.0  0.0  0.142372  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "4      0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "...    ...  ...  ...  ...       ...  ...  ...   ...      ...         ...  ...   \n",
       "42141  0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "42142  0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "42143  0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "42144  0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "42145  0.0  0.0  0.0  0.0  0.000000  0.0  0.0   0.0      0.0         0.0  ...   \n",
       "\n",
       "       yet  yeterday  yielded   yo        yr  yrs  yrsno  ysterday  zero  \\\n",
       "0      0.0       0.0      0.0  0.0  0.000000  0.0    0.0       0.0   0.0   \n",
       "1      0.0       0.0      0.0  0.0  0.000000  0.0    0.0       0.0   0.0   \n",
       "2      0.0       0.0      0.0  0.0  0.000000  0.0    0.0       0.0   0.0   \n",
       "3      0.0       0.0      0.0  0.0  0.139238  0.0    0.0       0.0   0.0   \n",
       "4      0.0       0.0      0.0  0.0  0.000000  0.0    0.0       0.0   0.0   \n",
       "...    ...       ...      ...  ...       ...  ...    ...       ...   ...   \n",
       "42141  0.0       0.0      0.0  0.0  0.135213  0.0    0.0       0.0   0.0   \n",
       "42142  0.0       0.0      0.0  0.0  0.000000  0.0    0.0       0.0   0.0   \n",
       "42143  0.0       0.0      0.0  0.0  0.000000  0.0    0.0       0.0   0.0   \n",
       "42144  0.0       0.0      0.0  0.0  0.000000  0.0    0.0       0.0   0.0   \n",
       "42145  0.0       0.0      0.0  0.0  0.128980  0.0    0.0       0.0   0.0   \n",
       "\n",
       "       zolpidem  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "42141       0.0  \n",
       "42142       0.0  \n",
       "42143       0.0  \n",
       "42144       0.0  \n",
       "42145       0.0  \n",
       "\n",
       "[42146 rows x 9633 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = pd.DataFrame(pn_history_vector_2.toarray(), columns=feature_names)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIM3zK0RQVA2"
   },
   "source": [
    "## 3. Machine Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyBLDELaQVA2"
   },
   "source": [
    "### 3.1 Machine Learning Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZPPmVSWQVA8",
    "outputId": "80436a2a-52fb-46d2-9e29-9aa86f9cb443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42146, 9633)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCQlVd-FfAgX"
   },
   "source": [
    "#### Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6Kcu9HoQVA8",
    "outputId": "8bd356f9-8c0b-4679-d223-b39cb2302c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the entire dtm data frame\n"
     ]
    }
   ],
   "source": [
    "missing_values = dtm.isnull().sum()\n",
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values[missing_values > 0].index\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values in the entire dtm data frame\")\n",
    "else:\n",
    "    # Print columns with missing values\n",
    "    print(\"Columns with missing values:\")\n",
    "    for column in columns_with_missing_values:\n",
    "        print(column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlaHFhsyQVA8"
   },
   "source": [
    "#### Outlier Detection\n",
    "- For outlier detection, we're using three method: Local Outlier Factor, and Isolation Forest together to select common outliers;\n",
    "- For each of these model, we set up the outlier percentage(contamination) to 2% since we're dealing with a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0Qo2vBk0QVA8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6039310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "# Use Local Outlier Factor model to detect outliers\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.02)\n",
    "\n",
    "y_pred_lof = lof.fit_predict(dtm)\n",
    "outlier_scores = lof.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rs6OQHdoQVA9",
    "outputId": "62c26cf5-e175-4093-af50-d003afe928cc"
   },
   "outputs": [],
   "source": [
    "# Use Isolation Forest outlier detection model\n",
    "iforest = IsolationForest(n_estimators=100, contamination=0.02)\n",
    "y_pred_if = iforest.fit_predict(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4eOc1OAQVA9",
    "outputId": "33655b5f-7e72-4b35-8c7c-74867a114db8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_out\n",
       " 0    40476\n",
       "-1     1654\n",
       "-2       16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the newly created columns to the dtm table\n",
    "dtm[\"y_pred_lof\"] = y_pred_lof\n",
    "dtm[\"y_pred_if\"] = y_pred_if\n",
    "\n",
    "# Converting the column values into binary -1 and 0, where -1 denotes outlier\n",
    "dtm[\"y_pred_lof_2\"] = np.where(dtm[\"y_pred_lof\"]<0, -1, 0)\n",
    "dtm[\"y_pred_if_2\"] = np.where(dtm[\"y_pred_if\"]<0, -1, 0)\n",
    "\n",
    "# Summing the outlier status\n",
    "dtm[\"all_out\"] = dtm.loc[:,[\"y_pred_if_2\",\"y_pred_lof_2\"]].sum(axis = 1)\n",
    "dtm[\"all_out\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hBm77yvyTBc",
    "outputId": "acb00632-d754-47ce-b29e-e3ed72697a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation forest outlier value counts:\n",
      "y_pred_if_2\n",
      " 0    41303\n",
      "-1      843\n",
      "Name: count, dtype: int64\n",
      "Local Outlier Factor outlier value counts: \n",
      "y_pred_lof_2\n",
      " 0    41303\n",
      "-1      843\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Isolation forest outlier value counts:\\n{dtm['y_pred_if_2'].value_counts()}\")\n",
    "print(f\"Local Outlier Factor outlier value counts: \\n{dtm['y_pred_lof_2'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN3Loxyq0C3b"
   },
   "source": [
    "- From the above distribution of outliers(denoted by -1) in each method, we can see that outliers detected by each method didn't overlap at all;\n",
    "- In the case, we're gonna stick with Isolation Forest since it's usually used for high dimension dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "m4prktG5QVA9"
   },
   "outputs": [],
   "source": [
    "# Eliminate outliers\n",
    "dtm.drop(dtm[dtm[\"y_pred_if_2\"] == -1].index, inplace=True)\n",
    "\n",
    "# Drop the columns added for outlier detection\n",
    "outlier_columns = ['y_pred_lof', 'y_pred_lof_2', 'y_pred_if', 'y_pred_if_2', 'all_out']\n",
    "dtm.drop(outlier_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOSdPj97QVA9"
   },
   "source": [
    "### 3.2 Dimensionality Reduction\n",
    "- We choose svd to perform dimensionality reduction considering pca is not suitable for dataset with large sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "HjFrb90OQVA9",
    "outputId": "c3ea9afc-f349-4c04-ce18-28fe620a8623"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example with TruncatedSVD\n",
    "svd = TruncatedSVD(n_components = 9633)  # Start with a larger number of components\n",
    "reduced_data = svd.fit_transform(dtm)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.cumsum(svd.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see from the above chart that 4000 features make up for 90% of the importance; 2000 features make up for 80% of the importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8Pn9gJrFQVA9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variance explained: 0.84\n"
     ]
    }
   ],
   "source": [
    "svd_1 = TruncatedSVD(n_components=2000)\n",
    "reduced_data_1 = svd_1.fit_transform(dtm)\n",
    "print(f\"Total variance explained: {np.sum(svd_1.explained_variance_ratio_):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDK5nt1XQVA9"
   },
   "source": [
    "### 3.3 Clustering\n",
    "- Use K-means for clustering, and since we have 10 labels in the dataset, we're trying to do 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hy00q6LzQVA-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a19dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a89670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a894c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a893a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a890d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a891f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a895e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a893a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a89670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x2adac6a89430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, ..., 4, 4, 4], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use K-means for clustering\n",
    "X1 = reduced_data_1\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(X1)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score:  0.04776969568770763\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Clustering\n",
    "silhouette_avg = silhouette_score(reduced_data_1, kmeans.labels_)\n",
    "print(\"Silhouette Score: \", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's check the k-means clustering with the original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_group_0\n",
      "{5: 29, 7: 2239}\n",
      "prediction_group_1\n",
      "{5: 740, 8: 68}\n",
      "prediction_group_2\n",
      "{3: 112, 5: 23, 8: 1823}\n",
      "prediction_group_3\n",
      "{3: 7740, 5: 1854, 9: 159}\n",
      "prediction_group_4\n",
      "{0: 395, 5: 3, 9: 5007}\n",
      "prediction_group_5\n",
      "{0: 6387, 1: 1, 2: 495, 4: 1, 5: 25}\n",
      "prediction_group_6\n",
      "{1: 555, 2: 1036, 5: 6}\n",
      "prediction_group_7\n",
      "{1: 3343, 5: 24, 6: 732, 8: 1, 9: 1}\n",
      "prediction_group_8\n",
      "{4: 796, 5: 5, 6: 3395}\n",
      "prediction_group_9\n",
      "{4: 4294, 5: 14}\n"
     ]
    }
   ],
   "source": [
    "result = kmeans.labels_\n",
    "\n",
    "# Create a dictionary for the count of data points for each label\n",
    "dictionary1 = {0: 2268, 1:808, 2:1958, 3:9753, 4:5405, 5:6909, 6:1597, 7:4101, 8:4196, 9:5151}\n",
    "\n",
    "start = 0\n",
    "prediction_groups = {}\n",
    "for idx, count in dictionary1.items():\n",
    "    prediction_groups[f'prediction_group_{idx}'] = result[start:start+count]\n",
    "    start += count\n",
    "\n",
    "# For each label, print it's cluster labels\n",
    "for label, value in prediction_groups.items():\n",
    "    print(label)\n",
    "    unique_values, counts = np.unique(value, return_counts=True)\n",
    "    print(dict(zip(unique_values, counts)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "- By looking at the silhouette score 0.047, which is pretty low, we think the clustering is not perfectly fit with our data; For non-standardized text, this outcome is not surprising\n",
    "- However, if we look at the labels correlation to the clusters, each label do correspond to mainly one cluster. We think the cluster performance is not that bad\n",
    "\n",
    "#### More Thoughts\n",
    "- We learned lots of machine learning techniques from this assignment, however, our solution to deal with the data is not the best\n",
    "- In reality, if we want to deal with physician notes, it's important that we format the notes, standardize the medical categories/process for the notes, and use medical dictionary to extract abbreviations\n",
    "- Moreover, it's worth considering using Large Language Models to summarize the notes first and then perform the ML techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
