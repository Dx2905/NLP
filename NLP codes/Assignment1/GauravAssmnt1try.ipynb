{"cells":[{"cell_type":"code","execution_count":8,"id":"eac14c12","metadata":{"id":"eac14c12","outputId":"491cb600-c217-46c9-ca85-b00069669f1e","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"error","timestamp":1706987234803,"user_tz":300,"elapsed":578,"user":{"displayName":"gaurav gaurav","userId":"04467603408697462699"}}},"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 3 fields in line 37158, saw 5\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8554175c4020>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Read the CSV file using pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/patient_notes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# notes = notes.sample(frac=0.01, random_state=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 37158, saw 5\n"]}],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.stem import PorterStemmer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","# Specify the file path\n","# file_path = \"/Users/p.mittal/Library/Mobile Documents/com~apple~CloudDocs/Roux/Courses/NLP/Assignments/Assignment 1 BOW/Subset_patient_notes.csv\"\n","\n","# Read the CSV file using pandas\n","notes = pd.read_csv('/content/patient_notes.csv')\n","# notes = notes.sample(frac=0.01, random_state=1)\n","\n","# Process the data\n","print(notes.shape)\n","notes.head()\n","\n","notes[\"case_num\"].value_counts()\n"]},{"cell_type":"code","execution_count":null,"id":"cc866e79","metadata":{"id":"cc866e79"},"outputs":[],"source":["# ---------------------------------------\n","# ---------------------------------------\n","# Step 2: Cretae a customized Tokenizer\n","# ---------------------------------------\n","# ---------------------------------------\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","\n","# ---------------------------------------\n","# Initialize stemmer and lemmatizer\n","# ---------------------------------------\n","# i. Case Conversion\n","notes['pn_history'] = notes['pn_history'].str.lower()\n","\n","# ii. Removing Punctuation and Special Characters\n","notes['pn_history'] = notes['pn_history'].str.replace(r'[^\\w\\s]', '', regex=True)\n","\n","\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","def custom_tokenizer(text):\n","    # Remove special characters and numbers\n","    text = re.sub(r'\\W|\\d', ' ', text)\n","#     # Define a regular expression pattern to identify and preserve age-related formats\n","#     age_pattern = r'\\b\\d{1,3}(?:(?:[-/]?[Yy](?:[Oo]?|[Ee][Aa][Rr]\\s?[Oo][Ll][Dd]))|(?:[-/]?\\d{1,3}(?:[Yy]\\.?[Oo])?))\\b'\n","\n","#     # Replace all non-age-related numbers with a space\n","#     text = re.sub(r'\\b(?!(?:[-/]?[Yy](?:[Oo]?|[Ee][Aa][Rr]\\s?[Oo][Ll][Dd]))|(?:[-/]?\\d{1,3}(?:[Yy]\\.?[Oo])?))\\d{1,3}\\b', ' ', text)\n","\n","#     # Remove special characters but keep numbers\n","#     text = re.sub(r'[^\\w\\s\\d]', ' ', text)\n","\n","\n","    # Tokenize text\n","    tokens = word_tokenize(text)\n","\n","    # Remove single-character tokens\n","    tokens = [token for token in tokens if len(token) > 2]\n","\n","    # Apply stemming and lemmatization\n","#     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n","#     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    return tokens"]},{"cell_type":"code","execution_count":null,"id":"771bce2b","metadata":{"id":"771bce2b"},"outputs":[],"source":["import collections\n","\n","# Create a frequency dictionary\n","def create_frequency_dictionary(tokens):\n","    frequency_dict = collections.Counter(tokens)\n","    return frequency_dict\n","\n","# Create a bigram dictionary\n","def create_bigram_dictionary(tokens):\n","    bigram_dict = collections.Counter(zip(tokens, tokens[1:]))\n","    return bigram_dict\n","\n","# Tokenize and preprocess the 'pn_history' column\n","tokenized_pn_history = notes['pn_history'].apply(custom_tokenizer)\n","\n","# Flatten the list of tokenized notes into a single list of tokens\n","all_tokens = [token for sublist in tokenized_pn_history for token in sublist]\n","\n","# Create frequency dictionary\n","frequency_dictionary = create_frequency_dictionary(all_tokens)\n","\n","# Create bigram dictionary\n","bigram_dictionary = create_bigram_dictionary(all_tokens)\n","\n","# Save frequency dictionary to a text file\n","with open(\"frequency_dictionary.txt\", \"w\") as f:\n","    for word, count in frequency_dictionary.items():\n","        f.write(f\"{word}: {count}\\n\")\n","\n","# Save bigram dictionary to a text file\n","with open(\"bigram_dictionary.txt\", \"w\") as f:\n","    for bigram, count in bigram_dictionary.items():\n","        f.write(f\"{bigram[0]} {bigram[1]}: {count}\\n\")\n"]},{"cell_type":"code","execution_count":null,"id":"3cfd142f","metadata":{"id":"3cfd142f"},"outputs":[],"source":["import symspellpy\n","import re\n","\n","# Create a SymSpell object\n","symspell = symspellpy.SymSpell()\n","\n","# Load a frequency dictionary\n","dictionary_path = \"frequency_dictionary.txt\"\n","bigram_path = \"bigram_dictionary.txt\"\n","\n","# Load the dictionary and bigram data\n","symspell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","symspell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n","\n","# Function for spelling correction using SymSpell\n","def correct_spelling_symspell(text):\n","    # Tokenize the input text using regular expressions (split on non-word characters)\n","    words = re.findall(r'\\w+', text)\n","\n","    # Correct the spelling of each word using SymSpell\n","    corrected_words = [symspell.lookup(word, symspellpy.Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)[0].term for word in words]\n","\n","    # Remove any colons that might have been added during correction\n","    corrected_words = [word.replace(':', '') for word in corrected_words]\n","\n","    # Join the corrected words back into a text with spaces\n","    corrected_text = ' '.join(corrected_words)\n","\n","    return corrected_text\n","\n","# Apply the spelling correction function to the 'pn_history' column\n","notes['pn_history'] = notes['pn_history'].apply(correct_spelling_symspell)\n","print(notes['pn_history'])\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a224a8ec","metadata":{"id":"a224a8ec","outputId":"29aee529-17cb-4d19-97ee-cdca3248ed17"},"outputs":[{"name":"stdout","output_type":"stream","text":["(421, 3744)\n","Number of features: ['_ve', 'aaf', 'abd', 'abdmoninal', 'abdomen', 'abdomianl', 'abdominal', 'abdominalpelvic', 'abdoninal', 'able']\n"]},{"name":"stderr","output_type":"stream","text":["/Users/gaurav/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"data":{"text/plain":["CountVectorizer()"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","# ---------------------------------------\n","# Apply customized tokenizer\n","# ---------------------------------------\n","\n","vectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n","\n","\n","# ---------------------------------------\n","# Fit and transform the 'pn_history' column\n","# ---------------------------------------\n","pn_history_vector = vectorizer.fit_transform(notes['pn_history'])\n","\n","# ---------------------------------------\n","# Print the shape of the vectorized 'pn_history' column\n","# ---------------------------------------\n","print(pn_history_vector.shape)\n","\n","# ---------------------------------------\n","# Get the feature names (tokens)\n","# ---------------------------------------\n","feature_names = vectorizer.get_feature_names()\n","print(\"Number of features:\", feature_names[0:10])\n","\n","\n","# ---------------------------------------\n","# Parameters of CountVectorizer()\n","# ---------------------------------------\n","CountVectorizer()"]},{"cell_type":"code","execution_count":null,"id":"03c0c63c","metadata":{"id":"03c0c63c","outputId":"c2f13624-93ea-456a-d803-1f9054392305"},"outputs":[{"name":"stdout","output_type":"stream","text":["318\n","318\n","['these', 'cry', 'your', 'become', 'mostly', 'whereas', 'over', 'why', 'system', 'few', 'take', 'which', 'anyhow', 'serious', 'ltd', 'none', 're', 'side', 'do', 'alone', 'them', 'will', 'move', 'must', 'either', 'cant', 'there', 'to', 'describe', 'in', 'what', 'empty', 'this', 'no', 'except', 'latterly', 'where', 'not', 'something', 'anyone', 'might', 'name', 'towards', 'is', 'against', 'de', 'wherein', 'anyway', 'only', 'upon', 'i', 'both', 'hers', 'whether', 'up', 'least', 'sometimes', 'therefore', 'eg', 'detail', 'via', 'until', 'twenty', 'enough', 'once', 'are', 'never', 'because', 'could', 'beyond', 'for', 'others', 'thereupon', 'hundred', 'by', 'less', 'my', 'two', 'thick', 'five', 'as', 'already', 'at', 'ten', 'ours', 'became', 'everyone', 'onto', 'everywhere', 'and', 'whither', 'whenever', 'everything', 'that', 'thus', 'fifty', 'ourselves', 'while', 'too', 'the', 'together', 'throughout', 'fire', 'again', 'how', 'can', 'he', 'else', 'amoungst', 'being', 'done', 'many', 'itself', 'first', 'most', 'interest', 'noone', 'also', 'has', 'perhaps', 'neither', 'other', 'whence', 'third', 'am', 'out', 'hereupon', 'top', 'among', 'formerly', 'nothing', 'keep', 'be', 'here', 'rather', 'you', 'due', 'indeed', 'bottom', 'us', 'someone', 'give', 'often', 'eleven', 'an', 'therein', 'thin', 'bill', 'me', 'very', 'whom', 'off', 'have', 'call', 'put', 'was', 'toward', 'seemed', 'fifteen', 'after', 'several', 'his', 'more', 'found', 'nine', 'ie', 'another', 'himself', 'some', 'him', 'somehow', 'go', 'each', 'below', 'amongst', 'thereafter', 'whereafter', 'con', 'fill', 'latter', 'further', 'with', 'such', 'nor', 'within', 'of', 'our', 'somewhere', 'see', 'sixty', 'though', 'back', 'nevertheless', 'etc', 'whose', 'mill', 'find', 'namely', 'mine', 'yet', 'full', 'ever', 'were', 'three', 'sometime', 'elsewhere', 'four', 'well', 'they', 'on', 'always', 'one', 'into', 'moreover', 'wherever', 'please', 'whoever', 'along', 'yourselves', 'now', 'forty', 'much', 'amount', 'becomes', 'herein', 'thence', 'than', 'otherwise', 'or', 'co', 'so', 'front', 'any', 'yourself', 'under', 'even', 'couldnt', 'whereupon', 'through', 'behind', 'she', 'it', 'twelve', 'between', 'hereby', 'her', 'last', 'whole', 'although', 'almost', 'since', 'six', 'next', 'whereby', 'part', 'every', 'inc', 'may', 'about', 'per', 'anything', 'around', 'yours', 'nowhere', 'myself', 'had', 'meanwhile', 'before', 'if', 'becoming', 'get', 'should', 'would', 'seeming', 'same', 'beside', 'eight', 'been', 'sincere', 'then', 'thru', 'however', 'former', 'thereby', 'seem', 'beforehand', 'whatever', 'we', 'all', 'un', 'themselves', 'those', 'above', 'when', 'its', 'down', 'nobody', 'who', 'afterwards', 'during', 'across', 'hereafter', 'but', 'hence', 'cannot', 'show', 'from', 'made', 'own', 'still', 'a', 'seems', 'besides', 'anywhere', 'hasnt', 'without', 'their', 'herself']\n","First few ORIGINAL words: ['_ve', 'aaf', 'abd', 'abdmoninal', 'abdomen', 'abdomianl', 'abdominal', 'abdominalpelvic', 'abdoninal', 'able', 'abnormal', 'abnormalities', 'abnormality', 'abnormals', 'abodminal', 'abominal', 'abortions', 'aborts', 'about', 'abovce', 'above', 'abrupt', 'absent', 'abuse', 'abuses', 'academically', 'accasionaly', 'accident', 'accidnet', 'accompainied', 'accompanied', 'accompoanied', 'ace', 'acetaminophen', 'ache', 'aches', 'achey', 'acheycrampy', 'achieved', 'achiness', 'aching', 'achy', 'acid', 'acitve', 'acive', 'acknowledged', 'acloholmo', 'acne', 'acompanied', 'acting', 'active', 'activemonogamous', 'activieis', 'activites', 'activities', 'activitiesdecreased', 'activity', 'actually', 'acute', 'acutley', 'acvtie', 'aday', 'added', 'adderal', 'adderall', 'adderral', 'addies', 'addition', 'additionally', 'additonal', 'aderol', 'aderral', 'adls', 'admin', 'administrative', 'administrator', 'admistrator', 'admit', 'admits', 'admitted', 'adopted', 'adversion', 'advil', 'advised', 'affected', 'affecting', 'afford', 'aforementioned', 'afraid', 'after', 'afternoon', 'afternoons', 'afterward', 'afterwards', 'aftre', 'again', 'age', 'agency', 'ages', 'aggervatingreleving']\n","---------\n","First few words after new stop list: ['_ve', 'aaf', 'abd', 'abdmoninal', 'abdomen', 'abdomianl', 'abdominal', 'abdominalpelvic', 'abdoninal', 'able', 'abnormal', 'abnormalities', 'abnormality', 'abnormals', 'abodminal', 'abominal', 'abortions', 'aborts', 'abovce', 'abrupt', 'absent', 'abuse', 'abuses', 'academically', 'accasionaly', 'accident', 'accidnet', 'accompainied', 'accompanied', 'accompoanied', 'ace', 'acetaminophen', 'ache', 'aches', 'achey', 'acheycrampy', 'achieved', 'achiness', 'aching', 'achy', 'acid', 'acitve', 'acive', 'acknowledged', 'acloholmo', 'acne', 'acompanied', 'acting', 'active', 'activemonogamous', 'activieis', 'activites', 'activities', 'activitiesdecreased', 'activity', 'actually', 'acute', 'acutley', 'acvtie', 'aday', 'added', 'adderal', 'adderall', 'adderral', 'addies', 'addition', 'additionally', 'additonal', 'aderol', 'aderral', 'adls', 'admin', 'administrative', 'administrator', 'admistrator', 'admit', 'admits', 'admitted', 'adopted', 'adversion', 'advil', 'advised', 'affected', 'affecting', 'afford', 'aforementioned', 'afraid', 'afternoon', 'afternoons', 'afterward', 'aftre', 'age', 'agency', 'ages', 'aggervatingreleving', 'aggrav', 'aggravate', 'aggravated', 'aggravates', 'aggravating']\n"]}],"source":["# ---------------------------------------\n","# ---------------------------------------\n","# Step 3: Modifying the Stop Words list\n","# ---------------------------------------\n","# ---------------------------------------\n","\n","# ---------------------------------------\n","# Print list of Stop words in CountVectorizer()\n","# ---------------------------------------\n","\n","stopwords = CountVectorizer(stop_words='english').get_stop_words()\n","len(stopwords)\n","\n","\n","\n","# ------------------------------------------------------------------------------\n","# Adding words to the stop words list\n","# (Note the choice of additional stop words is only for illustration)\n","# ------------------------------------------------------------------------------\n","\n","# nstopwords = list(stopwords) + ['abdomin', 'abdomen', 'abdonmin','age']\n","nstopwords = list(stopwords)\n","print(len(stopwords))\n","print(len(nstopwords))\n","print(nstopwords)\n","\n","# --------------------------------------------------------\n","# repeating the process with the new list of stopwords\n","# --------------------------------------------------------\n","\n","vectorizer2 = CountVectorizer(tokenizer=custom_tokenizer, stop_words=nstopwords)\n","\n","\n","# Fit and transform the 'pn_history' column\n","pn_history_vector = vectorizer2.fit_transform(notes['pn_history'])\n","\n","# Get the feature names (tokens)\n","feature_names2 = vectorizer2.get_feature_names()\n","\n","\n","print(\"First few ORIGINAL words:\", feature_names[0:100])\n","\n","print(\"---------\")\n","\n","print(\"First few words after new stop list:\", feature_names2[0:100])\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"633800c4","metadata":{"id":"633800c4","outputId":"8a0ef1f5-15a7-450e-db20-15976f11e3c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of DTM: (421, 3566)\n","Number of features: 3566\n","First 100 features: ['_ve' 'aaf' 'abd' 'abdmoninal' 'abdomen' 'abdomianl' 'abdominal'\n"," 'abdominalpelvic' 'abdoninal' 'able' 'abnormal' 'abnormalities'\n"," 'abnormality' 'abnormals' 'abodminal' 'abominal' 'abortions' 'aborts'\n"," 'abovce' 'abrupt' 'absent' 'abuse' 'abuses' 'academically' 'accasionaly'\n"," 'accident' 'accidnet' 'accompainied' 'accompanied' 'accompoanied' 'ace'\n"," 'acetaminophen' 'ache' 'aches' 'achey' 'acheycrampy' 'achieved'\n"," 'achiness' 'aching' 'achy' 'acid' 'acitve' 'acive' 'acknowledged'\n"," 'acloholmo' 'acne' 'acompanied' 'acting' 'active' 'activemonogamous'\n"," 'activieis' 'activites' 'activities' 'activitiesdecreased' 'activity'\n"," 'actually' 'acute' 'acutley' 'acvtie' 'aday' 'added' 'adderal' 'adderall'\n"," 'adderral' 'addies' 'addition' 'additionally' 'additonal' 'aderol'\n"," 'aderral' 'adls' 'admin' 'administrative' 'administrator' 'admistrator'\n"," 'admit' 'admits' 'admitted' 'adopted' 'adversion' 'advil' 'advised'\n"," 'affected' 'affecting' 'afford' 'aforementioned' 'afraid' 'afternoon'\n"," 'afternoons' 'afterward' 'aftre' 'age' 'agency' 'ages'\n"," 'aggervatingreleving' 'aggrav' 'aggravate' 'aggravated' 'aggravates'\n"," 'aggravating']\n","Number of non-zero elements: 27904\n","Sparsity of DTM: 0.9814132683579277\n","   _ve  aaf  abd  abdmoninal  abdomen  abdomianl  abdominal  abdominalpelvic  \\\n","0  0.0  0.0  0.0         0.0      0.0        0.0        0.0              0.0   \n","1  0.0  0.0  0.0         0.0      0.0        0.0        0.0              0.0   \n","2  0.0  0.0  0.0         0.0      0.0        0.0        0.0              0.0   \n","3  0.0  0.0  0.0         0.0      0.0        0.0        0.0              0.0   \n","4  0.0  0.0  0.0         0.0      0.0        0.0        0.0              0.0   \n","\n","   abdoninal  able  ...  yoaf       yof  yom  yono     young  younger  yowf  \\\n","0        0.0   0.0  ...   0.0  0.000000  0.0   0.0  0.137068      0.0   0.0   \n","1        0.0   0.0  ...   0.0  0.000000  0.0   0.0  0.000000      0.0   0.0   \n","2        0.0   0.0  ...   0.0  0.000000  0.0   0.0  0.000000      0.0   0.0   \n","3        0.0   0.0  ...   0.0  0.150779  0.0   0.0  0.000000      0.0   0.0   \n","4        0.0   0.0  ...   0.0  0.000000  0.0   0.0  0.000000      0.0   0.0   \n","\n","      yrago  yrs  ytears  \n","0  0.165733  0.0     0.0  \n","1  0.000000  0.0     0.0  \n","2  0.000000  0.0     0.0  \n","3  0.000000  0.0     0.0  \n","4  0.000000  0.0     0.0  \n","\n","[5 rows x 3566 columns]\n"]}],"source":["# ...\n","# ...\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Use TfidfVectorizer with custom tokenizer, stop words, and n-grams\n","vectorizer = TfidfVectorizer(\n","    tokenizer=custom_tokenizer,\n","    stop_words=stopwords,\n","    ngram_range=(1, 1)  # Include unigrams\n","#     max_df=0.85,         # Ignore terms that have a document frequency higher than 85%\n","#     min_df=5,            # Ignore terms that have a document frequency lower than 2 documents\n","#     max_features=10000   # Only consider the top 10,000 features ordered by term frequency across the corpus\n",")\n","\n","# Fit and transform the 'pn_history' column\n","pn_history_vector = vectorizer.fit_transform(notes['pn_history'])\n","\n","# Print the shape of the vectorized 'pn_history' column\n","print(\"Shape of DTM:\", pn_history_vector.shape)\n","\n","# Get the feature names (tokens)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Print the number of features (tokens)\n","print(\"Number of features:\", len(feature_names))\n","\n","with open('feature_names.txt', 'w') as file:\n","    file.write('\\n'.join(feature_names))\n","\n","# Print the first 100 features\n","print(\"First 100 features:\", feature_names[:100])\n","\n","# Describe the DTM from the dataframe (df) perspective\n","# Number of non-zero elements in the DTM\n","non_zero_elements = pn_history_vector.getnnz()\n","print(\"Number of non-zero elements:\", non_zero_elements)\n","\n","\n","# size_in_bytes = pn_history_vector.toarray().nbytes\n","print(\"Size in bytes:\", size_in_bytes)\n","\n","# Sparsity of the DTM\n","sparsity = 1.0 - (non_zero_elements / (pn_history_vector.shape[0] * pn_history_vector.shape[1]))\n","print(\"Sparsity of DTM:\", sparsity)\n","\n","# Create DTM with columns as tokens and rows as documents\n","pn_history_df_tfidf = pd.DataFrame(pn_history_vector.toarray(), columns=feature_names)\n","print(pn_history_df_tfidf.head())\n"]},{"cell_type":"code","execution_count":null,"id":"730e3c0b","metadata":{"id":"730e3c0b"},"outputs":[],"source":["## Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.ensemble import IsolationForest\n","\n","\n","# ML Pipeline\n","# a. Normalization\n","scaler = StandardScaler(with_mean=False)  # with_mean=False is necessary for sparse data\n","normalized_data = scaler.fit_transform(pn_history_vector)\n","\n","\n","# Outlier Detection using Isolation Forest\n","iso_forest = IsolationForest(n_estimators=300, contamination=0.02, random_state=0)\n","outliers = iso_forest.fit_predict(normalized_data)\n","\n","# Identifying the rows that are outliers\n","outlier_indices = np.where(outliers == -1)[0]\n","\n","# Count the number of outliers\n","num_outliers = np.sum(outliers == -1)\n","print(num_outliers)\n","\n","# Get the number of rows in the sparse matrix\n","num_rows = normalized_data.shape[0]\n","\n","# Calculate the percentage of outliers\n","percentage_outliers = (num_outliers / num_rows) * 100\n","\n","\n","print(f\"Percentage of outliers: {percentage_outliers:.4f}%\")\n","\n","\n","#remove outliers from your DataFrame:\n","# notes_filtered = notes.drop(index=outlier_indices)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"178c8a16","metadata":{"id":"178c8a16"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Example with TruncatedSVD\n","svd = TruncatedSVD(n_components=8000)  # Start with a larger number of components\n","reduced_data = svd.fit_transform(normalized_data)\n","\n","# Plot the cumulative explained variance\n","plt.figure(figsize=(10,6))\n","plt.plot(np.cumsum(svd.explained_variance_ratio_))\n","plt.xlabel('Number of components')\n","plt.ylabel('Cumulative explained variance')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"81b954a3","metadata":{"id":"81b954a3"},"outputs":[],"source":["# b. Dimension Reduction\n","n_components = 6000  # Number of components to keep\n","svd = TruncatedSVD(n_components=n_components)\n","reduced_data = svd.fit_transform(normalized_data)\n","print(f\"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}\")"]},{"cell_type":"code","execution_count":null,"id":"02f5bdcd","metadata":{"id":"02f5bdcd"},"outputs":[],"source":["# c. Clustering\n","n_clusters = 7  # Number of clusters\n","kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n","clusters = kmeans.fit_predict(reduced_data)\n","\n","# Add cluster labels to your dataframe\n","notes['cluster'] = clusters\n","\n","# Evaluate Clustering\n","silhouette_avg = silhouette_score(reduced_data, clusters)\n","print(\"Silhouette Score: \", silhouette_avg)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}